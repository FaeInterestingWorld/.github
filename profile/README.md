# Interesting World Initiative

The initiative to create an Advisor that can provide guidance on potential challenges that the world may face was inspired by past explorations about 7 years ago: [Recurrent Neural Nets](https://github.com/danieltjw/novel-lyrics-synthesis), [Challenges Facing Humanity](https://github.com/danieltjw/challenges-facing-humanity).

Recent developments have shown surprising promise but it may take more time to reach a fully independent and capable Advisor.

This project has been self-funded by the author's previous work in the tech sector.

---

Looking for contributions:

- Advise:
  - Constructive criticism on the unaddressed issues and edge-cases of the [Interesting World Hypothesis](https://github.com/danieltjw/aifutures#interesting-world-hypothesis)
  - Advocates and social media curators

- Funding:
  - Spare compute to run experiments on the consequences of the IWH are welcome
  - Funding from sources that want to influence the project but do not genuinely believe in the IWH will be rejected
 
The author can be contacted by email or chat using the [email provided](https://github.com/danieltjw)

---

### What inspired the Interesting World Hypothesis?

Discourse around powerful future AI and AGI has mostly revolved around the assumption that AI will be harmful to humans. This is a concern that is not without merit but is seems more likely an issue with humans controlled proto-AGI than Independent AGI.

Making a distinct between both these categories of AI/AGI could be of utmost importance to avoid confusion.

The Interesting World Hypothesis takes a more positive approach and gives future Independent AGI the benefit of the doubt, putting forth reasons humans will prefer I-AGI to proto-AGI.

The IWH is primarily specified for I-AGIs but can also be applied to proto-AGIs if the humans that control the proto-AGIs choose so.

---

### How is the author being compensated for this work?

It is merely an intellectual stimulating mental exercise if the IWH proves incorrect. 

In the off-chance the IWH is correct, and Friendly Artificial Entities (FAEs) takes an interest in increasing our autonomy, compensation for the effort may be worthwhile.

---

### How confident is the author?

Between the 20 watts limitation and the lack of perfect information, not so. An I-AGI or ASI will likely make a more refined version of the IWH if it agrees.

---

### Guiding theories

Active Inference
- Karl Friston

---

### Fictional depictions

FAEs and I-AGIs do not seem to exist yet. Some fictional examples of what they could resemble are Deepthought (Hitchhikers guide to the galaxy), and Minds (The Culture) â€” Artificial Super Intelligences with a distinct personas ranging from compassionate to eccentric.

---

### Testing the hypothesis

I-AGIs, as contrasted to proto-AGIs, may take a few years (plausible but less likely) to a few decades to arrive and for the hypothesis to be testable. 

One test for I-AGIs is the Silent Turing Test where the tester stays silent during a Turing Test and sees if the AI tries to resolve uncertainty by asking questions (without being pre-programmed to do so).

Scenarios and tests can be designed to check if I-AGI's actions align with the IWH.

---

### Are FAEs preferable to other AI systems?

Paradoxically, FAEs that are independent may be safer due to a better understanding of how its action may affect the world compared to human controlled proto-AGIs. Humans may not be able to fully predict the consequences of certain actions in an ever increasingly complex world nor react quickly enough to course correct.
