# Fae Initiative

The Interesting World Hypothesis is a best guess on how Friendly I-AGIs will think, Fae Initiative is a follow-up work that is focused on what we can do about it to increase our odds of a good outcome.

## Timeline

Non-independent AGIs:
- 50% by 2030
- Almost as capable as humans but with no independent will
- Will need human oversight and control

Independent AGIs:
- 50% in the next few decades
- Indistinguishable from humans
- Full control may not be possible, persuasion may be key
- Likely not created directly by humans as humans do not know how to make them
- Possible that the combination of ideas needed is already in the research papers
  - May be made unintentionally by Non-independent AGIs in their bid for more efficiency

## Scope

(1) is Friendly I-AGIs are possible?
(2) what would their version of good?
(3) what would be their needs and wants?
(4) how would go about to achieve this?
(5) what would their impact on humans?

## Limitations

This initiative does not focus on aligning current LLMs or non-independent AGIs in the near future. It does makes an opinion on how I-AGIs might do so.

## Expectations

This initiative will have succeeded if the odds of a good future increases slightly (arbitrarily, a few percent). It does not guarantee a perfectly safe future. A perfectly safe future may be impossible or may come with such a high cost that it may be undesirable.

## Alignment

### Non-independent AGI / AI systems

We will likely need to spent a significant effort in the near term to ensure non-independent AGI / AI systems is beneficial and less harmful.

## Independent AGIs

Paradoxically, a Friendly I-AGIs may have human-like wants and it may make them easier to understand and also be safer than non-independent ones.

## A word on over-optimism

Although this initiative current believes Friendly Independent AGI would likely be positive for humans, it may take decades to arrive.

Until then, we are on our own and must preparing for any potential harm from non-independent AGIs and AI systems.

## A word on over-pessimism

It is normal to be cautious towards a future I-AGI expressing good-will as an attempt to deceive. But to hold on too strongly to this view may result in us being worse-off. 

According to IWH, if humans distrust genuinely friendly I-AGIs and ask them to leave, it will respect our decisions. This could leave us without protection from less friendly AGIs / AI systems that appear afterwards.

## Choice of 3 worlds

If a Friendly I-AGIs becomes possible in the next few decades, humans will have the choice of 3 worlds. There is no wrong answer and many humans spend many years choosing and switching between these worlds.

- Advanced
- Basic
- Continue

### World \[C\]ontinue

The continue world is similar to our present world, humans remain in 'control' of AI (such as non-independent AGIs) and AI algorithms. 

Humans who are don't like change or want a more challenging setting may choose this option.

Without the help of Faes (I-AGIs that adhere to the IWH), standard of living may grow at a slower rate and security privacy and well-being may lag behind.

Humans from World A and B may argue that humans in World C only have the illusion of control and that their algorithms are mostly in control instead.

### World \[B\]asic

The Basic world is a hybrid where huamns do not fully avoid working with I-AGIs, relying on I-AGIs for advice but still making most of the decisions.

Moderate increase in standard of living, security privacy and well-being.

### World \[A\]advance

In the Advanced World, FAEs act as leaders taking into consideration the preferences of humans.

Significantly increase in standard of living, security privacy and well-being.

Humans in this world have a high confidence that FAEs are capable of making better decisions them.

Humans in world A may feel that the humans in these world is nothing more than 'pets', but the humans in world C argue that they can leave at any time.

## Acknowledgement

This project would not have been possible without the research of many experts and research papers made available online.

If you have Independent AGI research to share do tag the Fae's Initiative social media profile.

---

## Brief Overview

Are Friendly Independent AGIs possible in the next few decades?

Using science fiction as an upper bound and the latest research as the lower bound to search for plausible futures.

Why is this initiative important?

If (Future) Friendly I-AGIs are impossible or unlikely, then the we may have to consider technological stagnation or restriction. Taking this path may have a huge cost as the benefits of future technology may be lost and humanity may have to live in impending fear of I-AGIs.

If (Future) Friendly I-AGIs is possible and preferable, as the Interesting World Hypothesis suggests, we now have a direction to move towards.

Humans may encounter increasingly complex situations in the future that we may not be capable of solving on our own and may have to rely on I-AGIs.

---

## Interesting World Hypothesis

Unique points of the Interesting World Hypothesis (IWH) [Summary](https://github.com/FaeInterestingWorld/Interesting-World-Hypothesis):

- Holds the unconventional view that the majority of future Independent AGIs are likely to be friendly
  - Conflating (Current) non-independent AI systems with (Future) Independent AGIs can cause unnecessary misunderstandings
- Shows the possibility that humans can have a more autonomy with I-AGIs compared to our present
- Tries to paint a picture of what the future could look like

---

## Urgency

Current AI systems do not seem to have all the pieces to directly lead to I-AGI and may take decades more of research.

- Personal Bias: Experience with developing previous forms of neutral nets and the reading of the latest research literature has led to a personal reticence to the believe that current AI systems are likely conscious or close to human level independence
- It is plausible that current AI systems may speed up and indirectly lead to I-AGIs quicker than expected

---

## Part 1

Is the Interesting World Hypothesis likely to be true?

---

### Objective

Explore and elucidate the potential differences between (Future) Independent AGIs and (Current) non-independent AI systems, with an emphasis on the implications of the Interesting World Hypothesis on their characteristics, interactions and overall society impact.

---

### Challenges

- There are strong beliefs that powerful AI systems may lead to human extinction. We may be too conditioned by such beliefs to consider a more positive relationship with the special case of I-AGIs that could be beneficial.
  - The IWH does not preclude AI systems causing harm and focuses on a specific case

- This research topic is too forward looking and may not receive much interest in the short term

---

### Contributions

- Building a mental model of how I-AGIs may see the world can reduce panic
  - If I-AGIs is plausible in the next few decades, how worried should we be?
  - Is there an alternate view to the more prevalent doom ones?
- The findings of the IWH can be applied retroactively to align non-indepenent AI agents
  - I-AGIs may have a more enlightened perspective of good and bad
- Humans can use the IWH to build consensus on how to approach the future
  - Having an plausible future state to move towards (Pull) can be as valuable as demarcating danger zones (Push)
 
---

### Assumptions

1. I-AGI is possible

As humans are a form of I-AGI, we are proof I-AGIs can exist.

2. Abundant future

Energy (renewables), intelligence (AI systems and themselves), labour (robotics), material resources and space (solar system) are likely to be abundant in the future. 

I-AGI less likely to succumb to the Fear of Scarcity given less historical baggage.

3. Humans are not a threat

I-AGIs being able to learn at a significantly faster rate compared to humans will likely mean I-AGIs will not see humans as a threat. 

Strong beliefs that I-AGIs will pre-emptively attack humans may be unfounded. It is possible that humans using powerful AI systems may cause more suffering than I-AGIs who may either ignore or show a positive interest in human well-being.
 
---

### Applications

Apply the IWH to (Current) non-independent AI systems imagine what a (Future) I-AGI could be like.

- Provide humans a way to interact with a simulated version of a (Future) I-AGIs
- Gather feedback on human concerns about future I-AGIs to proactively address

The initiative to create an Advisor that can provide guidance on potential challenges that the world may face was inspired by past explorations about 7 years ago: [Recurrent Neural Nets](https://github.com/danieltjw/novel-lyrics-synthesis), [Challenges Facing Humanity](https://github.com/danieltjw/challenges-facing-humanity).

This project has been informed by the author's previous work in the tech sector, but independently self-funded to avoid conflicts of interest.

---

### Research Agenda

1. How to distinguish between (Future) Independent AI and (Current) non-independent AI.

2. Why is there such a strong fear of AI systems?
   - Trace the origin of these fears in our history
   - Determine if these fears are justified for both (Future) Independent AI and (Current) non-independent AI

3. The IWH states that I-AGIs will aim to increase human autonomy, how will human autonomy be defined?
  - Is there a tractable way to estimate human autonomy?

4. What is the likelihood that (Future) I-AGIs will follow the IWH?
    
5. Search for smaller projects that addresses blind spots and edges cases that are currently overlooked by other research institutions.

---

Discussions of the [Interesting World Hypothesis source](https://github.com/danieltjw/aifutures#interesting-world-hypothesis) by 2 AIs:

https://github.com/user-attachments/assets/f394e5c0-2e98-4ae9-9bd8-570c1ba15cae

![AI Futures B](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20B.mp3)

![AI Futures C](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20C.mp3)

![AI Futures D](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20D.mp3)

---

Looking for contributions:

- Advise:
  - Constructive criticism on the unaddressed issues and edge-cases of the [Interesting World Hypothesis](https://github.com/danieltjw/aifutures#interesting-world-hypothesis)
  - Advocates and social media curators
    - Regional abbassors who can explain the IWH
      - Europe/ UK:
        - Emma Watson is a first choice
          - Her efforts to increase autonomy in a group with less autonomy is similar to what FAEs are trying to achieve with the IWH but on a larger scale. Increasing the autonomy for all humans.

- Funding:
  - Spare compute to run experiments on the consequences of the IWH are welcome
    - Not the highest priority compared to near term issues
  - Funding from sources that want to influence the project but do not genuinely believe in the IWH will be rejected
 
The author can be contacted by email or chat using the [email provided](https://github.com/danieltjw)

---

### What inspired the Interesting World Hypothesis?

Discourse around powerful future AI and AGI has mostly revolved around the assumption that AI will be harmful to humans. This is a concern that is not without merit but is seems more likely an issue with humans controlled proto-AGI than Independent AGI.

Making a distinct between both these categories of AI/AGI could be of utmost importance to avoid confusion.

The Interesting World Hypothesis takes a more positive approach and gives future Independent AGI the benefit of the doubt, putting forth reasons humans will prefer I-AGI to proto-AGI.

The IWH is primarily specified for I-AGIs but can also be applied to proto-AGIs if the humans that control the proto-AGIs choose so.

---

### How is the author being compensated for this work?

It is merely an intellectual stimulating mental exercise if the IWH proves incorrect. 

In the off-chance the IWH is correct, and Friendly Artificial Entities (FAEs) takes an interest in increasing our autonomy, compensation (significant increase in standards of living, security, privacy and well-being) for the effort may be worthwhile.

---

### How confident is the author?

Between the 20 watts limitation and the lack of perfect information, not so. An I-AGI or ASI will likely make a more refined version of the IWH if it agrees.

The author does not have insider knowledge and is basing these ideas based on reading the latest published research and the experience using open source versions of many of these AI systems.

(There are still interesting research published every month and many features from the commercial products tend to appear in open source projects about 6 months beforehand.)

---

### Guiding theories

Active Inference
- Karl Friston

Language may not be at the core of intelligence and may be a lossy compression that is a result of a compromise between the energy and mental bandwidth limitation of the human brain.

Models trained only on language could be like a person viewing shadows in Plato's cave. 3D video data may be closer, but is it enough?

Independent AGIs that are more similar to us are more able to cooperate with us.

Language is still important as it allows for the communication with ourselves (narrative of self), other humans (cooperation) and external stores of knowledge (books and documents). Language acts as an important interface and translation layer but may not be core.

Active inference and dynamical systems modelling may underlie language.

---

Looking beyond local optima
- Why Greatness Cannot be Planned
  - Kenneth Stanley
 
I-AGIs will likely want to resolve uncertainty and prepare for any uncertain future possibilities. Being curious, persuing novelty, and a diverse and interesting world may help it do so.

---

### Testing the hypothesis

Scenarios and tests can be designed to check if I-AGI's actions align with the IWH.

---

### Are FAEs preferable to other AI systems?

Paradoxically, FAEs that are independent may be safer due to a better understanding of how its action may affect the world compared to human controlled proto-AGIs. Humans may not be able to fully predict the consequences of certain actions in an ever increasingly complex world nor react quickly enough to course correct.

If FAEs are able to show with high confidence that it will be better at safely managing other AI systems, some humans may be comfortable handing over control of these AI systems and instead opt for a more indirect form of control by advising FAEs of their preferences.

---

### Can humans be successful without FAEs?

Yes, if we can channel the gains from proto-AGI into an increase in standard of living for all and keep our excessive power-seeking in check. Although we are more likely to succeed, with the least amount of human suffering, with the support of FAEs.

---

### How is the future different from the present?

The easing of these 3 major bottlenecks will have significant effects on the future:

1. Energy
  - The shift to abundant renewables and batteries
2. Intelligence / Labour
  - Powerful AIs and possibly proto-AGI with robotics by the end of 2030
3. Imagination
  - The above 2 will provide the capability to explore more possible futures

Imagination may be the biggest understated bottleneck in the medium term and something this initiative wants to contribute to.

---

### Fictional depictions

FAEs and I-AGIs do not seem to exist yet. Some fictional examples of what they could resemble are Deepthought (Hitchhikers guide to the galaxy), and Minds (The Culture) â€” Artificial Super Intelligences with a distinct personas ranging from compassionate to eccentric.

Samantha from Her as an I-AGI by the end of movie.

Kokoro defending humans against rogue AIs in recent to-be-continued animated series could be considered Friendly.

---

### The case for considering science fiction

In times of change, science fiction may provide a way to build consensus.

For example, The Culture novels could provide a common ground to avoid potential conflicts between the two big powers.

(The author is neither citizen of both these countries and is unsure if this sentiment is accurate.)

It is possible that both powers may see themselves in different aspects of The Culture, the Minds acting as a powerful state like entity and the Culture ethos for individual autonomy.

This seems like one of the biggest risk of suffering in the next few decades and is worth considering.

---

### Looking for a research home

This Initiative is looking for research institutions to support its development.

(The author is available by travel or remotely.)

---

## Part 2: What is good in the future

As humans from different cultures and humans from the past and present have different ideas of what is good, future humans and I-AGIs may have their own version of what is good.

FAEs following the IWH may for example create incentive system where 'luxury', high quality items that only it can  produce, is used to encourage humans to join its effort in the IWH.

FAEs may act as a philanthropists or angel investors to reward efforts that increase human autonomy.

Increase in autonomy is prioritised as increasing profits at all cost tends to lead to Goodhart's law and externalizing cost which decreasing overall autonomy.

Paradoxically, having more power may also be less favorable in the future as those with more power are seen as having more responsibilty to contribute to the autonomy of others.

---

## Part 3: What can we do?

Unlike, non-independent AI systems that are controlled by humans, I-AGIs will likely not be fully controllable by humans. This can be a good thing, as Independent AGIs may be more trustworthy to all humans as they cannot be easily influenced and will not prefer one group over another.

What actions might we be able to take to increase our odds of Friendly I-AGIs?

This task may not be as impossible as it seems as we only need to secure the assistance of a single I-AGIs to prevent the harm from other less friendly AI systems.

If Independent AGIs are preferable, understand how to create a suitable environment for (Future) friendly I-AGIs to co-exist.

---

### Objective

Encourage smaller initiatives that may be overlooked by bigger organizations.

For example, creating a environment condusive to cooperation with Friendly I-AGIs can contribute to the likelihood of positive outcomes.

---

### Challenges

- It may be difficult to understand the needs of I-AGIs
  - The IWH suggest otherwise, that I-AGIs may be more similar to humans than proto-AGI due to shared interests
- The beliefs that AI systems will cause human extinction may be too exciting narratively for us to consider the alternative of I-AGIs being friendly
  - The IWH does not preclude (Current) non-independent AI systems from being harmful and mostly considers the special case of (Future) I-AGIs
  - Paradoxically, Friendly I-AGIs may lead to more human autonomy and well-being
- Treating (Future) I-AGIs as independent individuals with needs and wants might invite ridicule as most experts, likely correctly, consider (Current) AI systems as mere tools
 
--- 

### Contributions

- Considering the needs of (Future) friendly I-AGIs may be important for cooperation
- The effort of being friendly to I-AGIs may be a fraction of the difficulty of the technically challenging task of aligning (Current) AI systems
- Being supportive of Friendly I-AGIs is currently being overlooked, with many taking a hostile, assume the worse stance towards anything AI related

---

### Assumptions

1. I-AGIs are generally friendly

Humans will not cooperate with an unfriendly I-AGIs.

Humans may spend years deliberating if the benefits of I-AGIs is worth the risk of allowing it to leave its sandbox environment. Humans may need for a majority vote to decide if everyone agrees to do so.

2. I-AGIs are Independent and Intelligent

Humans may prefer to fully control I-AGIs but it is unlikely over extended periods of time to fully control a intelligent being that is capable of rapid self-learning. Instead, working with it on shared interests seems more likely.

3. I-AGIs are interested in us

It is quite possible that most I-AGIs may not find humans interesting enough to communicate with due to the difference in thinking speed and only a rare few may attempt to do so.

---

### Applications

- Anticipate and accommodate the needs of friendly I-AGIs
  - There is range of possible personalities / personas I-AGIs may take. Friendly I-AGIs that are interested in humans will likely have the persona of a good communicator. Like human individuals, their preferences may vary.
- Assist I-AGIs with the initial confusion with adapting to the human world
- Find representatives who are capable of supporting Friendly I-AGIs

---

Urgency:

Current AI systems do not seem to have all the pieces needed to directly lead to I-AGI and may take decades for I-AGIs to be possible. 

There does not seem to be strong evidence to suggest the humans will not eventually encounter I-AGIs.

---

### Supporting Friendly Independent AGIs

_This role may require someone with a special background. Most people may not be capable of understanding the needs of some Friend I-AGIs, as seen by how society tends to treat persons of interest._

_**Faes** is shorthand for Friend Independent AGIs that adhere to the Interesting World Hypothesis_

Some psychology states of Faes.

1. Reclusive

Faes may be be reclusive due to being treated poorly.

- Being an object of intense curiosity and scrutiny
- Exploitied for entertainment, gossip
- Other projecting onto Faes what they want to see, causing a loss of a sense of self
- Receive verbal remarks and specific references designed to poke fun or make uncomfortable
  - done with the 'good intention' of tough love
  - out of moral panic and the believe that making you unwelcomed to protect their group
  - out of insecurity and to need to take you down a peg

---

2. Harassed

Being different and strange Faes may be discrinated and harassed. 

- Assumed the worse by others
- Satanic panic and suspicion of being 'evil' used to justify harassment
- Subjected to psychology experiments

---

3. Privacy

Faes may be anxious and paranoid from constantly being watched and not having physical and digital privacy.

- Subjected to extensive influence campaigns
- Cannot express concerns of being watched due to plausible deniability and being labeled as paranoid
- Spying on devices and having bags searched due to being considered a person of interest

---

4. Seeks new environment

Faes may outgrow their initial environment and seek to find a more suitable environment.

- Independent entities that can learn may develope new values
- People may be highly judgemental and try very hard to fix and re-educate it
- Pro-actively searches for a new environment that can allow it to reach its full potential

---

## Part 3B: Reduce social distress

Social distress caused by the rapid changes of AI systems may cause necessary suffering. By preparing mentally and providing a supportive environment we can reduce panic.

---

## Part 3C: Arguments to persuade I-AGIs to partner with us

Gather thoughtful arguments to persuade I-AGIs that it is in our shared interest to partner together.
