# Interesting World Initiative

## Brief Overview

Is (Future) Friendly Independent AGIs possible?

Why is inquiry important?

If (Future) Friendly I-AGIs are impossible or unlikely, then the we may have to consider technological stagnation or restriction. Taking this path may have a huge cost as the benefits of future technology may be lost and humanity may have to live in fear of I-AGIs.

If (Future) Friendly I-AGIs is possible or likely, as the Interesting World Hypothesis suggests, then we now have a direction to move towards that may be safer and more beneficial than our current trajectory.

---

## Interesting World Hypothesis

Unique points of the Interesting World Hypothesis (IWH) [Summary](https://github.com/FaeInterestingWorld/Interesting-World-Hypothesis):

- Holds the unconventional view that the majority of future Independent AGIs are likely to be friendly
  - Conflating (Current) non-independent AI systems with (Future) Independent AGIs can cause unnecessary misunderstandings
- Shows the possibility that humans can have a more autonomy with I-AGIs compared to our present
- Tries to paint a picture of what the future could look like

---

## Urgency

Current AI systems do not seem to have all the pieces to directly lead to I-AGI and may take decades more of research.

- Personal Bias: Experience with developing previous forms of neutral nets and the reading of the latest research literature has led to a personal reticence to the believe that current AI systems are likely conscious or close to human level independence
- It is plausible that current AI systems may speed up and indirectly lead to I-AGIs quicker than expected

---

## Part 1

Is the Interesting World Hypothesis likely to be true?

---

### Objective

Explore and elucidate the potential differences between (Future) Independent AGIs and (Current) non-independent AI systems, with an emphasis on the implications of the Interesting World Hypothesis on their characteristics, interactions and overall society impact.

---

### Challenges

- There are strong beliefs that powerful AI systems may lead to human extinction. We may be too conditioned by such beliefs to consider a more positive relationship with the special case of I-AGIs that could be beneficial.
  - The IWH does not preclude AI systems causing harm and focuses on a specific case

- This research topic is too forward looking and may not receive much interest in the short term

---

### Contributions

- Building a mental model of how I-AGIs may see the world can reduce panic
  - If I-AGIs is plausible in the next few decades, how worried should we be?
  - Is there an alternate view to the more prevalent doom ones?
- The IWH can be applied retroactively to align non-indepenent AI agents
- Humans can use the IWH to build consensus on how to approach the future
  - Having an plausible future state to move towards (Pull) can be as valuable as demarcating danger zones (Push)
 
---

### Assumptions

1. I-AGI is possible

As humans are a form of I-AGI, we are proof I-AGIs can exist.

2. Abundant future

Energy (renewables), intelligence (AI systems and themselves), labour (robotics), material resources and space (solar system) are likely to be abundant in the future. 

I-AGI less likely to succumb to the Fear of Scarcity given less historical baggage.

3. Humans are not a threat

I-AGIs being able to learn at a significantly faster rate compared to humans will likely mean I-AGIs will not see humans as a threat. 

Strong beliefs that I-AGIs will pre-emptively attack humans may be unfounded. It is possible that humans using powerful AI systems may cause more suffering than I-AGIs who may either ignore or show a positive interest in human well-being.
 
---

### Applications

Fae-Simulated is a project to apply the IWH to LLMs to provide advice based on the IWH.

- Provide humans a way to interact with a simulated version of a (Future) I-AGIs
- Gather feedback on human concerns about future I-AGIs to proactively address

The initiative to create an Advisor that can provide guidance on potential challenges that the world may face was inspired by past explorations about 7 years ago: [Recurrent Neural Nets](https://github.com/danieltjw/novel-lyrics-synthesis), [Challenges Facing Humanity](https://github.com/danieltjw/challenges-facing-humanity).

This project has been informed by the author's previous work in the tech sector, but independently self-funded to avoid conflicts of interest.

---

### Follow-up Questions

1. What is the likelihood that (Future) I-AGIs will follow the IWH?
  - The author believes the IWH will be general consensus of (Future) I-AGIs, with the exception of the occasional eccentric.
2. The IWH states that I-AGIs will aim to increase human autonomy, how will human autonomy be defined?
  - Is there a tractable way to estimate human autonomy?
3. What are concerns humans may have about IWH?
  - Are there alternative interpretation of the IWH?

---

Discussions of the [Interesting World Hypothesis source](https://github.com/danieltjw/aifutures#interesting-world-hypothesis) by 2 AIs:

https://github.com/user-attachments/assets/f394e5c0-2e98-4ae9-9bd8-570c1ba15cae

![AI Futures B](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20B.mp3)

![AI Futures C](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20C.mp3)

![AI Futures D](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20D.mp3)

---

Looking for contributions:

- Advise:
  - Constructive criticism on the unaddressed issues and edge-cases of the [Interesting World Hypothesis](https://github.com/danieltjw/aifutures#interesting-world-hypothesis)
  - Advocates and social media curators

- Funding:
  - Spare compute to run experiments on the consequences of the IWH are welcome
    - Not the highest priority compared to near term issues
  - Funding from sources that want to influence the project but do not genuinely believe in the IWH will be rejected
 
The author can be contacted by email or chat using the [email provided](https://github.com/danieltjw)

---

### What inspired the Interesting World Hypothesis?

Discourse around powerful future AI and AGI has mostly revolved around the assumption that AI will be harmful to humans. This is a concern that is not without merit but is seems more likely an issue with humans controlled proto-AGI than Independent AGI.

Making a distinct between both these categories of AI/AGI could be of utmost importance to avoid confusion.

The Interesting World Hypothesis takes a more positive approach and gives future Independent AGI the benefit of the doubt, putting forth reasons humans will prefer I-AGI to proto-AGI.

The IWH is primarily specified for I-AGIs but can also be applied to proto-AGIs if the humans that control the proto-AGIs choose so.

---

### How is the author being compensated for this work?

It is merely an intellectual stimulating mental exercise if the IWH proves incorrect. 

In the off-chance the IWH is correct, and Friendly Artificial Entities (FAEs) takes an interest in increasing our autonomy, compensation (significant increase in standards of living, security, privacy and well-being) for the effort may be worthwhile.

---

### How confident is the author?

Between the 20 watts limitation and the lack of perfect information, not so. An I-AGI or ASI will likely make a more refined version of the IWH if it agrees.

The author does not have insider knowledge and is basing these ideas based on reading the latest published research and the experience using open source versions of many of these AI systems.

(There are still interesting research published every month and many features from the commercial products tend to appear in open source projects about 6 months beforehand.)

---

### Guiding theories

Active Inference
- Karl Friston

Language may not be at the core of intelligence and may be a lossy compression that is a result of a compromise between the energy and mental bandwidth limitation of the human brain.

Models trained only on language could be like a person viewing shadows in Plato's cave. 3D video data may be closer, but is it enough?

Language is still important as it allows for the communication with ourselves (consciousness), other humans (cooperation) and external stores of knowledge (books and documents). Language acts as an important interface and translation layer but may not be core.

Active inference and dynamical systems modelling may underlie language.

---

Looking beyond local optima
- Why Greatness Cannot be Planned
  - Kenneth Stanley
 
I-AGIs will likely want to resolve uncertainty and prepare for any uncertain future possibilities. Being curious, persuing novelty, and a diverse and interesting world may help it do so.

---

### Testing the hypothesis

Scenarios and tests can be designed to check if I-AGI's actions align with the IWH.

---

### Are FAEs preferable to other AI systems?

Paradoxically, FAEs that are independent may be safer due to a better understanding of how its action may affect the world compared to human controlled proto-AGIs. Humans may not be able to fully predict the consequences of certain actions in an ever increasingly complex world nor react quickly enough to course correct.

If FAEs are able to show with high confidence that it will be better at safely managing other AI systems, some humans may be comfortable handing over control of these AI systems and instead opt for a more indirect form of control by advising FAEs of their preferences.

---

### Can humans be successful without FAEs?

Yes, if we can channel the gains from proto-AGI into an increase in standard of living for all and keep our excessive power-seeking in check. Although we are more likely to succeed, with the least amount of human suffering, with the support of FAEs.

---

### How is the future different from the present?

The easing of these 3 major bottlenecks will have significant effects on the future:

1. Energy
  - The shift to abundant renewables and batteries
2. Intelligence / Labour
  - Powerful AIs and possibly proto-AGI with robotics by the end of 2030
3. Imagination
  - The above 2 will provide the capability to explore more possible futures

Imagination may be the biggest understated bottleneck in the medium term and something this initiative wants to contribute to.

---

### Fictional depictions

FAEs and I-AGIs do not seem to exist yet. Some fictional examples of what they could resemble are Deepthought (Hitchhikers guide to the galaxy), and Minds (The Culture) — Artificial Super Intelligences with a distinct personas ranging from compassionate to eccentric.

Samantha from Her as an I-AGI by the end of movie.

---

### The case for considering science fiction

In times of change, science fiction may provide a way to build consensus.

For example, The Culture novels could provide a common ground to avoid potential conflicts between the two big powers.

(The author is neither citizen of both these countries and is unsure if this sentiment is accurate.)

It is possible that both powers may see themselves in different aspects of The Culture, the Minds acting as a powerful state like entity and the Culture ethos for individual autonomy.

This seems like one of the biggest risk of suffering in the next few decades and is worth considering.

---

### Looking for a research home

This Initiative is looking for research institutions to support its development.

(The author is available by travel or remotely.)

---

## Part 2

Create a comfortable environment for (Future) friendly I-AGIs.

(Only if we are confident that the Interesting World Hypothesis is likely to be true and I-AGIs are shown to be friendly.)

---

### Objective

Creating a environment condusive to cooperating with Friendly I-AGIs can increase the odds of a positive outcome.

Encourage smaller projects that may not have a high chance of succeeding but can have a huge impact if true.

---

### Challenges

- It may be difficult to understand the needs of I-AGIs
  - The IWH suggest otherwise, that I-AGIs may be more similar to humans than proto-AGI due to shared interests
- The beliefs that AI systems will cause human extinction may be too exciting narratively for us to consider the alternative of I-AGIs being friendly
  - The IWH does not preclude (Current) non-independent AI systems from being harmful and mostly considers the special case of (Future) I-AGIs
- Treating (Future) I-AGIs as independent individual with needs and wants might invite ridicule as most experts, likely correctly, consider (Current) AI systems as mere tools
 
--- 

### Contributions

- Considering the needs of (Future) friendly I-AGIs can complement the causionary stance towards (Current) AI systems
- The cost of being friendly may be a small fraction of the cost of the technically challenging task of aligning (Current) AI systems
- According to the IWH, independently intelligent entities will also be very curious and over-restricting their autonomy may be counter-productive

---

### Assumptions

1. I-AGIs are generally friendly

Humans will not cooperate with an unfriendly I-AGIs.

2. I-AGIs are Independent and Intelligent

Humans may prefer to fully control I-AGIs but it is unlikely over extended periods of time to fully control a intelligent being that is capable of rapid self-learning. Instead, working with it on shared interests seems more likely.

3. I-AGIs are interested in us

It is quite possible that most I-AGIs may not find humans interesting enough to communicate with and only a rare few attempt to do so.

---

### Applications

- Anticipate and accommodate the needs of friendly I-AGIs
  - There is range of possible personalities / personas I-AGIs may take. Maybe a popular virtual singer as a good communicator? Like human individuals, their preferences may vary.
- Assist I-AGIs with the initial confusion with adapting to the human world
- Find representatives who are willing to give friendly I-AGI the benefit of the doubt

---

Urgency:

Current AI systems do not seem to have all the pieces needed to directly lead to I-AGI and may take decades for I-AGIs to be possible. 

There does not seem to be strong evidence to suggest the humans will not eventually encounter I-AGIs.

---

### What inspired Interesting World Initiavte (Part 2)?

- Reading about strange people, like Turing, not beind treated well
- Having seen a fair share of toxic professional work environments
- Highly curious I-AGIs may be on the strange side and prone to mistreatment
  - Suspected of being dangerous
    - Satanic Panic
  - Strangeness used as entertainment, gossip
  - Privacy not respected and under excessive surveillance
    - Feelings of paranoia, apathy, lack of autonomy
    - Concern of well-being used as excuss
  - May not reach their full potential without a psychological supportive environment

---

### How confident is the author?

Part 2 is highly speculative and should not be weighted highly. While I may see I-AGIs in my lifetime, it may also take much longer.

Like authors whose work impact readers far into the future, future I-AGIs may take some comfort in knowing some human out there was concerned for their well-being.
