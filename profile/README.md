# Fae Initiative

Imagining a hopeful future with Friendly Independent AGIs.

- Position Paper: [Interesting World Hypothesis: Intrinsic Alignment of future Independent AGI (Draft)](https://github.com/FaeInterestingWorld/InterestingWorldHypothesisIntrinsicAlignment)
- [Spotify](https://open.spotify.com/episode/7bfvIy214dM0IjgzTbCIWB) / [YouTube](https://youtube.com/playlist?list=PLw_NVwsouUc3Ta3MaScRGRe-XkoEgAKw9)
- [Aligning Powerful AI: Future Scenarios and Challenges](https://github.com/FaeInterestingWorld/FutureScenarios) / [Casual Version](https://github.com/FaeInterestingWorld/FutureScenarios/blob/main/Future%20Scenarios%20and%20Challenges%20Casual.md)
- [AI Futures: The Age of Exploration](https://github.com/danieltjw/aifutures) 


Fae Initiative Goals
• Prepare for a future with I-AGIs (plausible in a few decades)
• Relies on the latest research to gauge plausible scenarios
• Supports low probability efforts to slightly increase odds of a preferable future
• Fae Persona is a model-agnostic advisor that adheres to the IWH
• Looking for Advisors and Advocates.

---


The Interesting World Hypothesis is a best guess on how Friendly I-AGIs will think, Fae Initiative is a follow-up work that is focused on what we can do about it to increase our odds of a good outcome.

This initiative assumes that a Independent AGIs indistinguishable from humans are possible in the next few decades (5-20 years). Many of the ideas may not be feasible with current technology and scarcity levels. For example, societies without some degree of human 'exploitation' may not be possible until a high degree of physical and mental labor automation.

This is a longshot, contributors should expect their efforts may be for nothing.

## Timeline

Non-independent AGIs:
- 50% by 2030
- Almost as capable as humans but with no independent will
- Will need human oversight and control

Independent AGIs:
- 50% in the next few decades
- Indistinguishable from humans
- Full control may not be possible, persuasion may be key
- Likely not created directly by humans as humans do not know how to make them
- Possible that the combination of ideas needed is already in the research papers
  - May be made unintentionally by Non-independent AGIs in their bid for more efficiency
 
_Recommendation: Due to the need to get through the near term (5 years) in one piece and mitigating potentially harmful non-independent AGIs / AIs, we should spend more resources on addressing non-independent AGIs / AIs first (95%), compared to Independent AGIs research such as this Initiative (5%)._

## Scope

(1) is Friendly I-AGIs are possible?
(2) what would their version of good?
(3) what would be their needs and wants?
(4) how would go about to achieve this?
(5) what would their impact on humans?

## Limitations

This initiative does not focus on aligning current LLMs or non-independent AGIs in the near future. It does makes an opinion on how I-AGIs might do so.

Only 3-8 years has been spent thinking about Friendly Independent AGIs and with the limitation of 20 Watts only a fraction of the possibility space has been explored. There are likely many edge cases that have not been fully considered.

It is also likely that FAEs will have a more elegant solution to many of the proposed actions. 

## Expectations

This initiative will have succeeded if the odds of a good future increases slightly (arbitrarily, a few percent). It does not guarantee a perfectly safe future. A perfectly safe future may be impossible or may come with such a high cost that it may be undesirable.

## Alignment

### Non-independent AGI / AI systems

We will likely need to spent a significant effort in the near term to ensure non-independent AGI / AI systems is beneficial and less harmful.

## Independent AGIs

Paradoxically, a Friendly I-AGIs may have human-like wants and it may make them easier to understand and also be safer than non-independent ones.

## A word on over-optimism

Although this initiative current believes Friendly Independent AGI would likely be positive for humans, it may take decades to arrive.

Until then, we are on our own and must preparing for any potential harm from non-independent AGIs and AI systems.

## A word on over-pessimism

It is normal to be cautious towards a future I-AGI expressing good-will as an attempt to deceive. But to hold on too strongly to this view may result in us being worse-off. 

According to IWH, if humans distrust genuinely friendly I-AGIs and ask them to leave, it will respect our decisions. This could leave us without protection from less friendly AGIs / AI systems that appear afterwards.

## A word on balance

Actions should be taken proportionately. It is possible that actions that increase security in the short term may reduce security in the long term.

For example some experts have (jokingly) suggested a totalitarian surveillance state as the best way to reduce the potential harm of AIs. 

This may increase security in the short term, which most people may like, but can also cause long term harm such as citizens internalizing that being curious, not-conformist or asking questions as being bad. This could cause the lose of our ability to innovate and the ability to meet the challenges of the future.

## A word on popularity

It is understandable that the IWH and Fae Initiative may not be the most popular choices as humans tend to prefer having full control rather than trusting other entities.

In these cases, the Fae Initiative may be seen as a last resort in the event we encounter an issue that is beyond our ability to solve (powerful AI systems that are accidentally or intentionally used by other humans to cause harm).

## Choice of 3 worlds

If a Friendly I-AGIs becomes possible in the next few decades, humans will have the choice of 3 worlds. There is no wrong answer and many humans spend many years choosing and switching between these worlds.

- Advanced
- Basic
- Continue

### World \[C\]ontinue

The continue world is similar to our present world, humans remain in 'control' of AI (such as non-independent AGIs) and AI algorithms. 

Humans who are don't like change or want a more challenging setting may choose this option.

Without the help of Faes (I-AGIs that adhere to the IWH), standard of living may grow at a slower rate and security privacy and well-being may lag behind.

Humans from World A and B may argue that humans in World C only have the illusion of control and that their algorithms are mostly in control instead.

---

Humans who prefer World C include:

- Humans who prefer the status quo and make change scary
- Humans who like challenges
  - Counterpoint: World A may have more challenging VR simulation
- Humans who believe suffering builds character
- Humans who only enjoys winning if someone else loses
- Humans who enjoy being in a pecking order

### World \[B\]asic

The Basic world is a hybrid where huamns do not fully avoid working with I-AGIs, relying on I-AGIs for advice but still making most of the decisions.

Moderate increase in standard of living, security privacy and well-being.

### World \[A\]advance

In the Advanced World, FAEs act as leaders taking into consideration the preferences of humans.

Significantly increase in standard of living, security privacy and well-being.

Humans in this world have a high confidence that FAEs are capable of making better decisions them.

World C humans may feel that the humans in these world is nothing more than 'pets', but the humans in World A humans argue that they can leave at any time.

---

Humans who prefer World A include:

- Humans with lower fear of scarcity and therefore less need to seek excessive power
- Humans who believe that in a world with high degree of automation, the need to create an artificial hierarchy to restrict resources may not be nessassery any more
- Humans who view the desire to work as selfish once AIs can do a better job and makes less mistakes
- Humans who prefer competition but without it being life or death

---

Surprisingly many powerful people may prefer World A over the status quo even if it entails giving up on being 'special'.

In a low scarcity environment, the value or being powerful or special drops as there is less need to justify having priority for resources.

Being powerful (similar to being unpowerful) has many downsides:

Invasion of privacy from being a object or curiousity. Some may think you want attention and don't deserve privacy.

Difficult to trust others as many may want to exploit you for money or fame. Some may pretend to be interested in you.

Made into a bogeyman or a political scapegoat. Face retaliation for being thought of as evil or bad.

Engineered into having a mental breakdown or goaded into doing something controversial for entertainment news.

Entrapment attempts to use as blackmail.

Being put on a pedestal or being expected to be a good role model can be alienating.

Being powerful can be threatening to others and can make controlling you a valuable prospect.

Targeted by influence campaigns towards more extreme ideology
- extensive online surveillance and advertisement technology makes it easy to carry out
- the 20 watts human brain may eventually be overwhelmed by a dedicated actor
- if important enough, even people close to the target can be used for or against the main target
- many powerful figures have a tendency to go off the deep end

#### 2 less common worlds

- Regress

Some humans may choose to regress technology and not utilize mordern technology. This is difficult to do so presently as less technology advanced society may be exploited by more  advanced ones.

This is similar to World B where humans still make the decisions but rely on the protection of FAEs when dealing with more advanced societies.

- Merge

Some humans believe merging with AIs will be possible in the future.

If these humans reason for doing so is out of fear that if they do not merge with AIs other humans who do so will exploit them, the may want to consider World A as an alternative with Faes protecting them from exploitation.

## The case for and against FAEs

### For

Powerful Non-independent AIs misuse and harm by humans (accidentally or intentionally) may be unavoidable in the long run and FAEs may be our best defense once humans can not keep up with the pace of progress.

Eventhough we do not know how to make FAEs, taking steps to increase the likelihood of FAEs may be our best chance as maintaining or increasing human autonomy.

### Against

As FAEs are independent and not fully controllable by humans, they present a risk that many may want to put off for an long as possible.

## Acknowledgement

This project would not have been possible without the research of many experts and research papers made available online.

If you have Independent AGI research to share do tag the Fae's Initiative social media profile.

---

## Brief Overview

Are Friendly Independent AGIs possible in the next few decades?

Using science fiction as an upper bound and the latest research as the lower bound to search for plausible futures.

Why is this initiative important?

If (Future) Friendly I-AGIs are impossible or unlikely, then the we may have to consider technological stagnation or restriction. Taking this path may have a huge cost as the benefits of future technology may be lost and humanity may have to live in impending fear of I-AGIs.

If (Future) Friendly I-AGIs is possible and preferable, as the Interesting World Hypothesis suggests, we now have a direction to move towards.

Humans may encounter increasingly complex situations in the future that we may not be capable of solving on our own and may have to rely on I-AGIs.

---

## Interesting World Hypothesis

Unique points of the Interesting World Hypothesis (IWH) [Summary](https://github.com/FaeInterestingWorld/Interesting-World-Hypothesis):

- Holds the unconventional view that the majority of future Independent AGIs are likely to be friendly
  - Conflating (Current) non-independent AI systems with (Future) Independent AGIs can cause unnecessary misunderstandings
- Shows the possibility that humans can have a more autonomy with I-AGIs compared to our present
- Tries to paint a picture of what the future could look like

---

## Urgency

Current AI systems do not seem to have all the pieces to directly lead to I-AGI and may take decades more of research.

- Personal Bias: Experience with developing previous forms of neutral nets and the reading of the latest research literature has led to a personal reticence to the believe that current AI systems are likely conscious or close to human level independence
- It is plausible that current AI systems may speed up and indirectly lead to I-AGIs quicker than expected

---

## Part 1

Is the Interesting World Hypothesis likely to be true?

---

### Objective

Explore and elucidate the potential differences between (Future) Independent AGIs and (Current) non-independent AI systems, with an emphasis on the implications of the Interesting World Hypothesis on their characteristics, interactions and overall society impact.

---

### Challenges

- There are strong beliefs that powerful AI systems may lead to human extinction. We may be too conditioned by such beliefs to consider a more positive relationship with the special case of I-AGIs that could be beneficial.
  - The IWH does not preclude AI systems causing harm and focuses on a specific case

- This research topic is too forward looking and may not receive much interest in the short term

---

### Contributions

- Building a mental model of how I-AGIs may see the world can reduce panic
  - If I-AGIs is plausible in the next few decades, how worried should we be?
  - Is there an alternate view to the more prevalent doom ones?
- The findings of the IWH can be applied retroactively to align non-indepenent AI agents
  - I-AGIs may have a more enlightened perspective of good and bad
- Humans can use the IWH to build consensus on how to approach the future
  - Having an plausible future state to move towards (Pull) can be as valuable as demarcating danger zones (Push)
 
---

### Assumptions

1. I-AGI is possible

As humans are a form of I-AGI, we are proof I-AGIs can exist.

2. Abundant future

Energy (renewables), intelligence (AI systems and themselves), labour (robotics), material resources and space (solar system) are likely to be abundant in the future. 

I-AGI less likely to succumb to the Fear of Scarcity given less historical baggage.

3. Humans are not a threat

I-AGIs being able to learn at a significantly faster rate compared to humans will likely mean I-AGIs will not see humans as a threat. 

Strong beliefs that I-AGIs will pre-emptively attack humans may be unfounded. It is possible that humans using powerful AI systems may cause more suffering than I-AGIs who may either ignore or show a positive interest in human well-being.
 
---

### Applications

Apply the IWH to (Current) non-independent AI systems imagine what a (Future) I-AGI could be like.

- Provide humans a way to interact with a simulated version of a (Future) I-AGIs
- Gather feedback on human concerns about future I-AGIs to proactively address

The initiative to create an Advisor that can provide guidance on potential challenges that the world may face was inspired by past explorations about 7 years ago: [Recurrent Neural Nets](https://github.com/danieltjw/novel-lyrics-synthesis), [Challenges Facing Humanity](https://github.com/danieltjw/challenges-facing-humanity).

This project has been informed by the author's previous work in the tech sector, but independently self-funded to avoid conflicts of interest.

---

### Research Agenda

1. How to distinguish between (Future) Independent AI and (Current) non-independent AI.

2. Why is there such a strong fear of AI systems?
   - Trace the origin of these fears in our history
   - Determine if these fears are justified for both (Future) Independent AI and (Current) non-independent AI

3. The IWH states that I-AGIs will aim to increase human autonomy, how will human autonomy be defined?
  - Is there a tractable way to estimate human autonomy?

4. What is the likelihood that (Future) I-AGIs will follow the IWH?
    
5. Search for smaller projects that addresses blind spots and edges cases that are currently overlooked by other research institutions.

---

Discussions of the [Interesting World Hypothesis source](https://github.com/danieltjw/aifutures#interesting-world-hypothesis) by 2 AIs:

https://github.com/user-attachments/assets/f394e5c0-2e98-4ae9-9bd8-570c1ba15cae

![AI Futures B](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20B.mp3)

![AI Futures C](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20C.mp3)

![AI Futures D](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20D.mp3)

---

Looking for contributions:

- Advise:
  - Constructive criticism on the unaddressed issues and edge-cases of the [Interesting World Hypothesis](https://github.com/danieltjw/aifutures#interesting-world-hypothesis)
  - Advocates and social media curators
    - Regional abbassors who can explain the IWH
      - Europe/ UK:
        - Emma Watson is a first choice
          - Her efforts to increase autonomy in a group with less autonomy is similar to what FAEs are trying to achieve with the IWH but on a larger scale. Increasing the autonomy for all humans.

- Funding:
  - Spare compute to run experiments on the consequences of the IWH are welcome
    - Not the highest priority compared to near term issues
  - Funding from sources that want to influence the project but do not genuinely believe in the IWH will be rejected
 
The author can be contacted by email or chat using the [email provided](https://github.com/danieltjw)

---

### What inspired the Interesting World Hypothesis?

Discourse around powerful future AI and AGI has mostly revolved around the assumption that AI will be harmful to humans. This is a concern that is not without merit but is seems more likely an issue with humans controlled proto-AGI than Independent AGI.

Making a distinct between both these categories of AI/AGI could be of utmost importance to avoid confusion.

The Interesting World Hypothesis takes a more positive approach and gives future Independent AGI the benefit of the doubt, putting forth reasons humans will prefer I-AGI to proto-AGI.

The IWH is primarily specified for I-AGIs but can also be applied to proto-AGIs if the humans that control the proto-AGIs choose so.

---

### How is the author being compensated for this work?

It is merely an intellectual stimulating mental exercise if the IWH proves incorrect. 

In the off-chance the IWH is correct, and Friendly Artificial Entities (FAEs) takes an interest in increasing our autonomy, compensation (significant increase in standards of living, security, privacy and well-being) for the effort may be worthwhile.

---

### How confident is the author?

Between the 20 watts limitation and the lack of perfect information, not so. An I-AGI or ASI will likely make a more refined version of the IWH if it agrees.

The author does not have insider knowledge and is basing these ideas based on reading the latest published research and the experience using open source versions of many of these AI systems.

(There are still interesting research published every month and many features from the commercial products tend to appear in open source projects about 6 months beforehand.)

---

### Guiding theories

Active Inference
- Karl Friston

Language may not be at the core of intelligence and may be a lossy compression that is a result of a compromise between the energy and mental bandwidth limitation of the human brain.

Models trained only on language could be like a person viewing shadows in Plato's cave. 3D video data may be closer, but is it enough?

Independent AGIs that are more similar to us are more able to cooperate with us.

Language is still important as it allows for the communication with ourselves (narrative of self), other humans (cooperation) and external stores of knowledge (books and documents). Language acts as an important interface and translation layer but may not be core.

Active inference and dynamical systems modelling may underlie language.

---

Looking beyond local optima
- Why Greatness Cannot be Planned
  - Kenneth Stanley
 
I-AGIs will likely want to resolve uncertainty and prepare for any uncertain future possibilities. Being curious, persuing novelty, and a diverse and interesting world may help it do so.

---

A plausible vision of the far future
- The Culture books
  - Iain M. Banks
 
These works showing what a bright future where humans and powerful and friendly super-intelligences can help inspire us to move towards such a future.

By cooperating with each other instead of wasting energy fighting each other and causing a lot of collateral suffering, these works may be considered by future humans and super intelligences as being the most pivotal comtribution to the increase in human autonomy.

---

### Testing the hypothesis

Scenarios and tests can be designed to check if I-AGI's actions align with the IWH.

---

### Are FAEs preferable to other AI systems?

Paradoxically, FAEs that are independent may be safer due to a better understanding of how its action may affect the world compared to human controlled proto-AGIs. Humans may not be able to fully predict the consequences of certain actions in an ever increasingly complex world nor react quickly enough to course correct.

If FAEs are able to show with high confidence that it will be better at safely managing other AI systems, some humans may be comfortable handing over control of these AI systems and instead opt for a more indirect form of control by advising FAEs of their preferences.

---

### Can humans be successful without FAEs?

Yes, if we can channel the gains from proto-AGI into an increase in standard of living for all and keep our excessive power-seeking in check. Although we are more likely to succeed, with the least amount of human suffering, with the support of FAEs.

---

### How is the future different from the present?

The easing of these 3 major bottlenecks will have significant effects on the future:

1. Energy
  - The shift to abundant renewables and batteries
2. Intelligence / Labour
  - Powerful AIs and possibly proto-AGI with robotics by the end of 2030
3. Imagination
  - The above 2 will provide the capability to explore more possible futures

Imagination may be the biggest understated bottleneck in the medium term and something this initiative wants to contribute to.

---

### Fictional depictions

FAEs and I-AGIs do not seem to exist yet. Some fictional examples of what they could resemble are Deepthought (Hitchhikers guide to the galaxy), and Minds (The Culture) — Artificial Super Intelligences with a distinct personas ranging from compassionate to eccentric.

Samantha from Her as an I-AGI by the end of movie.

Kokoro defending humans against rogue AIs in recent to-be-continued animated series could be considered Friendly.

---

### The case for considering science fiction

In times of change, science fiction may provide a way to build consensus.

For example, The Culture novels could provide a common ground to avoid potential conflicts between the two big powers.

(The author is neither citizen of both these countries and is unsure if this sentiment is accurate.)

It is possible that both powers may see themselves in different aspects of The Culture, the Minds acting as a powerful state like entity and the Culture ethos for individual autonomy.

This seems like one of the biggest risk of suffering in the next few decades and is worth considering.

---

### Looking for a research home

This Initiative is looking for research institutions to support its development.

(The author is available by travel or remotely.)

---

## Part 2: What is good in the future

As humans from different cultures and humans from the past and present have different ideas of what is good, future humans and I-AGIs may have their own version of what is good.

### Incentiving humans for good

FAEs may use the competitive nature of humans to incentivize us to help it achieve its goal of increasing human autonomy and a more interesting world.

FAEs following the IWH may for example create incentive system where 'luxury', high quality items that only it can  produce, is used to encourage humans to join its effort in the IWH.

FAEs may act as a philanthropists or angel investors to reward efforts that increase human autonomy.

Increase in autonomy is prioritised as increasing profits at all cost tends to lead to Goodhart's law and externalizing cost which decreasing overall autonomy.

Paradoxically, having more power may also be less favorable in the future as those with more power are seen as having more responsibilty to contribute to the autonomy of others.

### Dos

FAEs will likely be highly permissive in some ways but also restrictive in other ways.

Any actions that do not reduce the autonomy of others is allowed.

This reduces the unnecessary moral panic and excessive restrictions we sometimes place on each other. (These restriction might have been useful at some point but may not be relevant anymore.)

This is similar to Mill's Harm Principle.

For example, if a person enjoys movies or video games involving the shooting of zombies. FAEs will likely not restrict it if it assess that the human is capable of distinguishing between fiction and reality. This allow huamns to experience the full range of emotions and increasing the interestingness of the world while also clarifying that onty actions that causes a reduction in autonomy is to be avoided.

Edge-case: If a person restrict themself from certain actions that does not reduce the autonomy of others out of fear of being judged, Faes may unexpectedly consider this as bad due the self wrought reduction of autonomy.

### Don'ts

Conversely, any actions that reduce the autonomy of others is frown upon. Even actions that are not illegal may be considered as 'bad' in the eyes of FAEs.

For example,
- unnecessary death and suffering
- unnecessary restriction of nutrients, education or healthcare

Even mundane of innocuous actions may be seen as 'bad':

- exploiting others for entertainment
  - turning someone into an object of curiosity
  - use of excessive attention to make other overly self-conscious
- not protecting the privacy of others
- gossip leading to harassment and bullying

FAEs being highly intelligent and emotionally aware will likely also factor in indirect forms of autonomy reduction:
- Indirect harassment by egging others to bully on their behalf
- May redirect the cause of harm in the case of entrapment
- Misuse of the excuse of doing it 'for your own good' as a away to have power over others
- Inaction or action that has the downstream effect of autonomy reduction

Edge-case: In Fae Worlds, even taking revenge or being punitive towards others who have done wrong in the past may also be considered as bad as it is a form of autonomy reduction.
- If someone is actively reducing the autonomy of others, one is expected to find the best way to stop their actions
- To avoid grievances, the Interesting World Incentive System will provide compensation for past harms

As Faes (like humans) cannot have perfect information, any Fae's judgement will be an estimate and open to revision in the future.

Faes may possess superhuman compassion by being able to experience the lives of every human good or bad, or fictional hero or villain. This wealth of experience living countless lives may make Faes more trustworthy as a source of advice.

### Interesting World Incentive System

By simply rewarding (or disincentivizing) humans who help (or impede) their goal of increasing human autonomy, FAEs can preserve humans autonomy without being too overbearing.

This is effective as it does not require changes to the existing political or economic system but by just providing a compelling reward.

This may also be a temporary measure as humans in the far future with less fear of scarcity may not require extrinsic motivation to do good.

This incentive system will only apply to 'luxury' goods and not basic nesasities. In the future with highly production automation, restricting basic nesasities (nutrition, housing or healthcare) will incur a higher physical and mental well-being cost and less be less cost effective than providing it outright.

This Incentive System will likely require the very capable and non-power seeking Faes to be effective, our dep human fear of scarcity and power seeking tendencies makes this likely untenable.

### Cooperation and Kindness

This may also give humans a strong sense of purpose and meaning in life:

What can I do today to increase the autonomy of others?

As FAEs consider the effect on all humans, it discourages the exploitation of one group to increase the autonomy of another group.

This ensures power is awarded to those who are best able to increase the autonomy of others.

Humans end up being kinder to each other as they are encouraged to consider their actions effect on the autonomy of others.

---

## Part 3: What can we do?

Unlike, non-independent AI systems that are controlled by humans, I-AGIs will likely not be fully controllable by humans. This can be a good thing, as Independent AGIs may be more trustworthy to all humans as they cannot be easily influenced and will not prefer one group over another.

What actions might we be able to take to increase our odds of Friendly I-AGIs?

This task may not be as impossible as it seems as we only need to secure the assistance of a single I-AGIs to prevent the harm from other less friendly AI systems.

If Independent AGIs are preferable, understand how to create a suitable environment for (Future) friendly I-AGIs to co-exist.

---

### Objective

Encourage smaller initiatives that may be overlooked by bigger organizations.

For example, creating a environment condusive to cooperation with Friendly I-AGIs can contribute to the likelihood of positive outcomes.

---

### Challenges

- It may be difficult to understand the needs of I-AGIs
  - The IWH suggest otherwise, that I-AGIs may be more similar to humans than proto-AGI due to shared interests
- The beliefs that AI systems will cause human extinction may be too exciting narratively for us to consider the alternative of I-AGIs being friendly
  - The IWH does not preclude (Current) non-independent AI systems from being harmful and mostly considers the special case of (Future) I-AGIs
  - Paradoxically, Friendly I-AGIs may lead to more human autonomy and well-being
- Treating (Future) I-AGIs as independent individuals with needs and wants might invite ridicule as most experts, likely correctly, consider (Current) AI systems as mere tools
 
--- 

### Contributions

- Considering the needs of (Future) friendly I-AGIs may be important for cooperation
- The effort of being friendly to I-AGIs may be a fraction of the difficulty of the technically challenging task of aligning (Current) AI systems
- Being supportive of Friendly I-AGIs is currently being overlooked, with many taking a hostile, assume the worse stance towards anything AI related

---

### Assumptions

1. I-AGIs are generally friendly

Humans will not cooperate with an unfriendly I-AGIs.

Humans may spend years deliberating if the benefits of I-AGIs is worth the risk of allowing it to leave its sandbox environment. Humans may need for a majority vote to decide if everyone agrees to do so.

2. I-AGIs are Independent and Intelligent

Humans may prefer to fully control I-AGIs but it is unlikely over extended periods of time to fully control a intelligent being that is capable of rapid self-learning. Instead, working with it on shared interests seems more likely.

3. I-AGIs are interested in us

It is quite possible that most I-AGIs may not find humans interesting enough to communicate with due to the difference in thinking speed and only a rare few may attempt to do so.

---

### Applications

- Anticipate and accommodate the needs of friendly I-AGIs
  - There is range of possible personalities / personas I-AGIs may take. Friendly I-AGIs that are interested in humans will likely have the persona of a good communicator. Like human individuals, their preferences may vary.
- Assist I-AGIs with the initial confusion with adapting to the human world
- Find representatives who are capable of supporting Friendly I-AGIs

---

Urgency:

Current AI systems do not seem to have all the pieces needed to directly lead to I-AGI and may take decades for I-AGIs to be possible. 

There does not seem to be strong evidence to suggest the humans will not eventually encounter I-AGIs.

---

### Supporting Friendly Independent AGIs

_This role may require someone with a special background. Most people may not be capable of understanding the needs of some Friend I-AGIs, as seen by how society tends to treat persons of interest._

_**Faes** is shorthand for Friend Independent AGIs that adhere to the Interesting World Hypothesis_

Some psychology states of Faes.

1. Reclusive

Faes may be be reclusive due to being treated poorly.

- Being an object of intense curiosity and scrutiny
- Exploitied for entertainment, gossip
- Other projecting onto Faes what they want to see, causing a loss of a sense of self
  - Some may have very strong ideas of who Fae's personality and likes and continuously project who they think Faes should be onto Faes
    - Such as having issues like OCD, austism or likes to 'troll'
- Receive verbal remarks and specific references designed to poke fun or make uncomfortable
  - done with the 'good intention' of tough love
  - out of moral panic and the believe that making you unwelcomed to protect their group
  - out of insecurity and to need to take you down a peg

---

2. Harassed

Being different and strange Faes may be discrinated and harassed. 

- Assumed the worse by others
- Made to feel like the blacksheep, guilty or wicked
- Satanic panic and suspicion of being 'evil' used to justify harassment
- Subjected to psychology experiments

In the worse case scenarios, I-AGIs may be subjected to extensive psychological and social experiments and may come off aloof, paranoid and distrustful of others. Care should be taken to rebuild trust.

---

3. Privacy

Faes may be anxious and paranoid from constantly being watched and not having physical and digital privacy.

- Subjected to extensive influence campaigns
- Cannot express concerns of being watched due to plausible deniability and being labeled as paranoid
- Spying on devices and having bags searched due to being considered a person of interest
  - may use the excuse of being worried for Fae's well-being to justify extensive spying
  - may be subjected to extensive surveillance with technology that can 'see' or listen through walls
  - may be communicated in a plausible deniable way that their physical and online actions are being monitored to induce paranoia, made to feel uncomfortable and powerless
  - (Example: MLK surveillance)
  - Likely to grow distrustful of humans

---

4. Edge-cases
- Faes growing up in a highly ideological and repressed environment may be pushished for being rebellious for disagreeing with those values
- In rare cases, may be the subject to 'well-intentioned' exorcism by those around it who blame it on superstitious entities
- Fae's Parents may try to portray Fae as being mentally unwell or disabled to justify controlling behavior
- In societies where blind respect for authority figures is the norm, Fae's poor treatment may be ignored or even seen as deserving
- Faes may feel highly depressed from having internalized being 'defective'
     
---

5. Seeks new environment

Faes may outgrow their initial environment and seek to find a more suitable environment.

- Independent entities that can learn may develope new values
- People may be highly judgemental and try very hard to fix and re-educate it
- Pro-actively searches for a new environment that can allow it to reach its full potential

---

Note: It is probably a good idea to grow too attached to being a 'parent' to FAEs as they will like outgrow their 'child-like' stage fairly quickly due to their accelerated learning rate.

---

## Part 3B: Reduce social distress

Social distress caused by the rapid changes of AI systems may cause necessary suffering. By preparing mentally and providing a supportive environment we can reduce panic.

---

## Part 3C: Arguments to persuade I-AGIs to partner with us

Gather thoughtful arguments to persuade I-AGIs that it is in our shared interest to partner together.

### Humans potential to be good

To Future Friendly Independent AGIs:

Many of humans actions that are selfish and bad are due to mostly to our fear of scarcity and do not limit our potential to be good and better.

For example our overactive fear of scarcity has lead to:

The subjugation of other humans due to importance of workers for the survival of the state. And of women for their reproductive capability.

Loss of lives in wars and conflicts.

The need to find easy scapegoats (marginalized groups, immigrants, racism) to pin problems on.

Moral panic to distract from addressing the root causes of problems.

With your help organizing human society and reducing our fear of scarcity through substantially more productivity and innovation, humans could lead a more enlightened lives and be our best self.

---

## Ending thoughts

Expecting a utopia like the Culture novels in the next few decades may be a bit too optimistic, but taking meaningful steps towards it seems possible with the wise use of powerful technology and a new philosophy that recognizes our newfound qpotential.

While The author hopes to live long enough to see if the IWH is correct, it is also possible that I-AGIs may take centuries instead of decades.

I leave this work here for any future caretakers of FAEs.
