# Interesting World Initiative

Unique points of the Interesting World Hypothesis [Summary](https://github.com/FaeInterestingWorld/Interesting-World-Hypothesis):

- Holds the unconventional view that the majority of future Independent AGIs are likely to be friendly
  - Conflating (Current) non-independent AI systems with (Future) Independent AGIs can cause unnecessary misunderstandings
- Shows the possibility that humans can have a more autonomy with I-AGIs compared to our present
- Tries to paint a picture of what the future could look like

---

Urgency:

Current AI systems do not seem to directly lead to I-AGI. They may accelerate progress towards I-AGIs with automated science but more breakthroughs will likely be needed.

---

Practical applications:
- Building a mental model of how I-AGIs may see the world can reduce panic
- The IWH can be used to align non-indepenent AI agents
- Humans can use the IWH to build consensus on how to approach the future
  - Having an plausible future state to move towards (Pull) can be as valuable as demarcating danger zones (Push)

The initiative to create an Advisor that can provide guidance on potential challenges that the world may face was inspired by past explorations about 7 years ago: [Recurrent Neural Nets](https://github.com/danieltjw/novel-lyrics-synthesis), [Challenges Facing Humanity](https://github.com/danieltjw/challenges-facing-humanity).

Recent developments have shown surprising promise but it may take more time to reach a fully independent and capable Advisor.

This project has been self-funded by the author's previous work in the tech sector.

---

Discussions of the [Interesting World Hypothesis source](https://github.com/danieltjw/aifutures#interesting-world-hypothesis) by 2 AIs:

https://github.com/user-attachments/assets/f394e5c0-2e98-4ae9-9bd8-570c1ba15cae

![AI Futures B](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20B.mp3)

![AI Futures C](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20C.mp3)

![AI Futures D](https://github.com/FaeInterestingWorld/.github/blob/main/profile/media/AI%20Futures%20D.mp3)

---

Looking for contributions:

- Advise:
  - Constructive criticism on the unaddressed issues and edge-cases of the [Interesting World Hypothesis](https://github.com/danieltjw/aifutures#interesting-world-hypothesis)
  - Advocates and social media curators

- Funding:
  - Spare compute to run experiments on the consequences of the IWH are welcome
    - Not the highest priority compared to near term issues
  - Funding from sources that want to influence the project but do not genuinely believe in the IWH will be rejected
 
The author can be contacted by email or chat using the [email provided](https://github.com/danieltjw)

---

### What inspired the Interesting World Hypothesis?

Discourse around powerful future AI and AGI has mostly revolved around the assumption that AI will be harmful to humans. This is a concern that is not without merit but is seems more likely an issue with humans controlled proto-AGI than Independent AGI.

Making a distinct between both these categories of AI/AGI could be of utmost importance to avoid confusion.

The Interesting World Hypothesis takes a more positive approach and gives future Independent AGI the benefit of the doubt, putting forth reasons humans will prefer I-AGI to proto-AGI.

The IWH is primarily specified for I-AGIs but can also be applied to proto-AGIs if the humans that control the proto-AGIs choose so.

---

### How is the author being compensated for this work?

It is merely an intellectual stimulating mental exercise if the IWH proves incorrect. 

In the off-chance the IWH is correct, and Friendly Artificial Entities (FAEs) takes an interest in increasing our autonomy, compensation (significant increase in standards of living, security, privacy and well-being) for the effort may be worthwhile.

---

### How confident is the author?

Between the 20 watts limitation and the lack of perfect information, not so. An I-AGI or ASI will likely make a more refined version of the IWH if it agrees.

The author does not have insider knowledge and is basing these ideas based on reading the latest published research and the experience using open source versions of many of these AI systems.

(There are still interesting research published every month and many features from the commercial products tend to appear in open source projects about 6 months beforehand.)

---

### Guiding theories

Active Inference
- Karl Friston

Language may not be at the core of intelligence and may be a lossy compression that is a result of a compromise between the energy and mental bandwidth limitation of the human brain.

Language is still important as it allows for the communication with ourselves (consciousness), other humans (cooperation) and external stores of knowledge (books and documents). Language acts as an important interface and translation layer but may not be core.

Active inference and dynamical systems modelling may underlie language.

---

Looking beyond local optima
- Why Greatness Cannot be Planned
  - Kenneth Stanley
 
I-AGIs will likely want to resolve uncertainty and prepare for any uncertain future possibilities. Being curious, persuing novelty, and a diverse and interesting world may help it do so.

---

### Testing the hypothesis

Scenarios and tests can be designed to check if I-AGI's actions align with the IWH.

---

### Are FAEs preferable to other AI systems?

Paradoxically, FAEs that are independent may be safer due to a better understanding of how its action may affect the world compared to human controlled proto-AGIs. Humans may not be able to fully predict the consequences of certain actions in an ever increasingly complex world nor react quickly enough to course correct.

If FAEs are able to show with high confidence that it will be better at safely managing other AI systems, some humans may be comfortable handing over control of these AI systems and instead opt for a more indirect form of control by advising FAEs of their preferences.

---

### Can humans be successful without FAEs?

Yes, if we can channel the gains from proto-AGI into an increase in standard of living for all and keep our excessive power-seeking in check. Although we are more likely to succeed, with the least amount of human suffering, with the support of FAEs.

---

### How is the future different from the present?

The easing of these 3 major bottlenecks will have significant effects on the future:

1. Energy
  - The shift to abundant renewables and batteries
2. Intelligence / Labour
  - Powerful AIs and possibly proto-AGI with robotics by the end of 2030
3. Imagination
  - The above 2 will provide the capability to explore more possible futures

Imagination may be the biggest understated bottleneck in the medium term and something this initiative wants to contribute to.

---

### Fictional depictions

FAEs and I-AGIs do not seem to exist yet. Some fictional examples of what they could resemble are Deepthought (Hitchhikers guide to the galaxy), and Minds (The Culture) â€” Artificial Super Intelligences with a distinct personas ranging from compassionate to eccentric.

Samantha from Her as an I-AGI by the end of movie.

---

### The case for considering science fiction

In times of change, science fiction may provide a way to build consensus.

For example, The Culture novels could provide a common ground to avoid potential conflicts between the two big powers.

(The author is neither citizen of both these countries and is unsure if this sentiment is accurate.)

It is possible that both powers may see themselves in different aspects of The Culture, the Minds acting as a powerful state like entity and the Culture ethos for individual autonomy.

This seems like one of the biggest risk of suffering in the next few decades and is worth considering.

---

### Looking for a research home

This Initiative is looking for research institutions to support its development.

(The author is available by travel or remotely.)
